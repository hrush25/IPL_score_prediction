import pandas as pd
import numpy as np
from sklearn import preprocessing

from google.colab import drive
drive.mount('/content/drive')

dataset = pd.read_csv('/content/drive/MyDrive/Datasets/0y clean data pre ipl.csv') 


import xgboost as xgb
#the outcome (dependent variable) has only a limited number of possible values. 
#Logistic Regression is used when response variable is categorical in nature.
from sklearn.linear_model import LogisticRegression
#A random forest is a meta estimator that fits a number of decision tree classifiers 
#on various sub-samples of the dataset and use averaging to improve the predictive 
#accuracy and control over-fitting.
from sklearn.ensemble import RandomForestClassifier
#a discriminative classifier formally defined by a separating hyperplane.
from sklearn.svm import SVC

dataset.head(2)

dataset= dataset.drop(['Unnamed: 0','extras','match_id', 'runs_off_bat'],axis = 1)
dataset.info()

a1 = dataset['venue'].unique()
a2 = dataset['batting_team'].unique()
a3 = dataset['bowling_team'].unique()
a4 = dataset['striker'].unique()
a5 = dataset['bowler'].unique()

def labelEncoding(data):
    dataset = pd.DataFrame(data)
    feature_dict ={}
    #features={}
    for feature in dataset:
        if dataset[feature].dtype==object:
            le = preprocessing.LabelEncoder()
            fs = dataset[feature].unique()
            le.fit(fs)
            dataset[feature] = le.transform(dataset[feature])
            feature_dict[feature] = le
            print(feature)
    return dataset

labelEncoding(dataset)

ip_dataset = dataset[['venue','innings', 'batting_team', 'bowling_team', 'striker', 'non_striker', 'bowler']]
b1 = ip_dataset['venue'].unique()
b2 = ip_dataset['batting_team'].unique()
b3 = ip_dataset['bowling_team'].unique()
b4 = ip_dataset['striker'].unique()
b5 = ip_dataset['bowler'].unique()
#ip_dataset.head(2)

from pandas.plotting import scatter_matrix

scatter_matrix(dataset,figsize=(15,18));

  

ip_dataset.drop('non_striker',axis=1,inplace=True)

X= ip_dataset.iloc[:,]
y = dataset.y

dataset = dataset.drop('y',axis = 1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)

X_test.shape

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)



from time import time 
# F1 score (also F-score or F-measure) is a measure of a test's accuracy. 
#It considers both the precision p and the recall r of the test to compute 
#the score: p is the number of correct positive results divided by the number of 
#all positive results, and r is the number of correct positive results divided by 
#the number of positive results that should have been returned. The F1 score can be 
#interpreted as a weighted average of the precision and recall, where an F1 score 
#reaches its best value at 1 and worst at 0.
from sklearn.metrics import f1_score

def train_classifier(clf, X_train, y_train):
    ''' Fits a classifier to the training data. '''
    
    # Start the clock, train the classifier, then stop the clock
    start = time()
    clf.fit(X_train, y_train)
    end = time()
    
    # Print the results
    print("Trained model in {:.4f} seconds".format(end - start))

    
def predict_labels(clf, features, target):
    ''' Makes predictions using a fit classifier based on F1 score. '''
    
    # Start the clock, make predictions, then stop the clock
    start = time()
    y_pred = clf.predict(features)
    
    end = time()
    # Print and return results
    print("Made predictions in {:.4f} seconds.".format(end - start))
    
    return f1_score(target, y_pred, pos_label='H',average='micro)
    # , sum(target == y_pred) / float(len(y_pred))


def train_predict(clf, X_train, y_train, X_test, y_test):
    ''' Train and predict using a classifer based on F1 score. '''
    
    # Indicate the classifier and the training set size
    print("Training a {} using a training set size of {}. . .".format(clf.__class__.__name__, len(X_train)))
    
    # Train the classifier
    train_classifier(clf, X_train, y_train)
    
    # Print the results of prediction for both training and testing
    f1, acc = predict_labels(clf, X_train, y_train)
    print( f1, acc)
    print("F1 score and accuracy score for training set: {:.4f} , {:.4f}.".format(f1 , acc))
    
    f1, acc = predict_labels(clf, X_test, y_test)
    print("F1 score and accuracy score for test set: {:.4f} , {:.4f}.".format(f1 , acc))

# Initialize the three models (XGBoost is initialized later)
clf_A = LogisticRegression(random_state = 42)
clf_B = SVC(random_state = 912, kernel='rbf')
#Boosting refers to this general problem of producing a very accurate prediction rule 
#by combining rough and moderately inaccurate rules-of-thumb
clf_C = xgb.XGBClassifier(seed = 82)

# train_predict(clf_A, X_train, y_train, X_test, y_test)
# print ()
# train_predict(clf_B, X_train, y_train, X_test, y_test)
# print ()
train_predict(clf_C, X_train, y_train, X_test, y_test)
# print ()

# ANN

Y_train= y_train
Y_test =y_test


# keras API uses Tensorflow as backend in Deep Learning
from keras.layers import Dense
# Feed Foreward NN
from keras.models import Sequential
# Model Optimizer - RMSProp
from keras.optimizers import RMSprop

# Feedforeward Dense Neural Network
def build_model():
    model = Sequential()
    # Input Layer - I - Take values of neurons as (2)pow n form.
    model.add(Dense(units = 64, activation='relu', input_shape = [len(X.keys())]))
    # Hidden Layer - I
    model.add(Dense(units = 128, activation='relu'))
    # Hidden Layer - II
    model.add(Dense(units = 256, activation='relu'))
    # Output Layer 
    model.add(Dense(units = 1))

    # Optimizers - RMSProp
    # Alpha = Learning rate (< 1)
    optimizers = RMSprop(learning_rate= 0.001)

    # Regression : Loss function - 'mean_squared_error'
    model.compile(loss = 'mean_squared_error', optimizer = optimizers, metrics = ['mean_squared_error', 
                                                                                  'mean_absolute_error'])
    
    return model

model = build_model()

model.summary()

# epochs = Number of Iterations
# Batch size = 20, A small sample / batch sample taken for iterations in each epoch 
# validation_splot = 15% of samples of overall data from training dataset for validation of model in each epoch 
history = model.fit(X_train, Y_train, epochs = 400, batch_size=40, validation_split = 0.15)

pd.DataFrame(history.history)

pd.DataFrame(history.history)[['mean_squared_error','val_mean_squared_error']].plot()

# Evaluate - Model Metrics 
model.evaluate(X_test, Y_test)

prediction = model.predict(X_test)

from sklearn.metrics import r2_score
r2_score(Y_test, prediction)
